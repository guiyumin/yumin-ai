+++
title = 'Google 白皮书：提示词工程的艺术与科学'
date = 2025-01-09T10:00:00-08:00
draft = false
tags = ['AI', 'LLM', '提示词工程', 'Google', '机器学习']
+++

Google 发布了一份关于提示词工程的综合白皮书，作者是 Lee Boonstra（2024 年 9 月）。本文提炼了这份 65 页文档的核心见解——一份从大语言模型获得更好结果的实用指南。

---

## 1. 引言

谈到大语言模型的输入和输出，文本提示词（有时伴随图像等其他模态）是模型用来预测特定输出的输入。

**你不需要成为数据科学家或机器学习工程师——每个人都可以写提示词。**

然而，撰写最有效的提示词可能很复杂。许多因素会影响其效果：你使用的模型、模型的训练数据、模型配置、用词选择、风格、语气、结构和上下文。

因此，提示词工程是一个**迭代过程**。不充分的提示词可能导致模糊、不准确的回复，阻碍模型提供有意义输出的能力。

---

## 2. 什么是提示词工程？

记住大语言模型是如何工作的：**它是一个预测引擎**。模型接收顺序文本作为输入，并根据训练数据预测下一个 token 应该是什么。大语言模型反复执行此操作，将每个预测的 token 添加到序列中以预测下一个。

当你写提示词时，你正在尝试让大语言模型预测正确的 token 序列。

**提示词工程**是设计高质量提示词以引导大语言模型产生准确输出的过程。这包括反复调整以找到最佳提示词、优化提示词长度，以及评估写作风格和结构与任务的关系。

提示词可用于各种任务：文本摘要、信息提取、问答、文本分类、语言或代码翻译、代码生成，以及代码文档和推理。

---

## 3. 大语言模型输出配置

选择模型后，你需要确定模型配置。大多数大语言模型都有各种控制输出的配置选项。有效的提示词工程需要为你的任务进行最优设置。

### 3.1 输出长度

一个重要设置是**生成的 token 数量**。生成更多 token 需要更多计算，导致更高的能耗、可能更慢的响应时间和更高的成本。

**重要提示**：减少输出长度不会让大语言模型在风格上更简洁——它只是让大语言模型在达到限制后停止预测。如果你需要简短输出，你还需要相应地设计你的提示词。

### 3.2 采样控制

大语言模型并不是正式预测单个 token。相反，它们预测下一个 token 可能是什么的**概率**。然后对这些概率进行采样以确定输出 token。

**Temperature**、**top-K** 和 **top-P** 是决定如何处理预测 token 概率的最常见设置。

#### 3.2.1 Temperature（温度）

Temperature 控制 token 选择的**随机程度**：

- **Temperature 0**（贪婪解码）：确定性——总是选择最高概率的 token
- **低 Temperature**（0.1-0.3）：更确定性、更事实性的回复
- **高 Temperature**（0.7-1.0）：更多样、更有创意、更意外的结果
- **非常高**（>1.0）：所有 token 变得同等可能

#### 3.2.2 Top-K 和 Top-P

这些采样设置限制预测的下一个 token 来自具有最高预测概率的 token。

**Top-K 采样**选择最可能的前 K 个 token。更高的 top-K 意味着更有创意和多样的输出；更低的 top-K 意味着更受限和更事实性的输出。Top-K 为 1 等同于贪婪解码。

**Top-P 采样**（核采样）选择累积概率不超过 P 的顶部 token。P = 0 表示只选最可能的 token（贪婪解码）；P = 1 表示考虑词汇表中的所有 token。

#### 3.2.3 综合运用

当 temperature、top-K 和 top-P 都可用时，满足 top-K 和 top-P 条件的 token 成为候选，然后应用 temperature 从这些候选中采样。

**极端设置会使其他配置失效**：Temperature = 0 使 top-K 和 top-P 无关；Top-K = 1 使 temperature 和 top-P 无关；Top-P = 0 使 temperature 和 top-K 无关。

**推荐的起始点：**

- 连贯、适度创意：Temperature 0.2，Top-P 0.95，Top-K 30
- 特别有创意：Temperature 0.9，Top-P 0.99，Top-K 40
- 较少创意、更事实性：Temperature 0.1，Top-P 0.9，Top-K 20
- 单一正确答案（数学）：Temperature 0

---

## 4. 提示词技术

大语言模型经过调优以遵循指令，并在大量数据上训练。但它们并不完美——你的提示词越清晰，大语言模型就能越好地预测正确的文本。

### 4.1 通用提示词 / 零样本（Zero-Shot）

**零样本**提示词是最简单的类型。它只提供任务描述和一些让大语言模型开始的文本。输入可以是问题、故事开头或指令。"零样本"意味着"没有示例"。

例如，你可以要求模型将电影评论分类为正面、中性或负面，而不先展示任何示例。模型使用其训练来理解你想要什么。

**专业提示**：对分类任务使用低 temperature（0.1），因为不需要创意。

### 4.2 单样本和少样本提示词（One-Shot & Few-Shot）

当零样本不起作用时，在提示词中提供示例。

**单样本提示词**：提供一个示例供模型模仿。

**少样本提示词**：提供多个示例展示要遵循的模式。

**需要多少示例？** 这取决于任务的复杂性、示例的质量和模型的能力。一般规则：少样本提示词至少使用**3-5 个示例**。更复杂的任务可能需要更多；输入长度限制可能需要更少。

**选择示例的准则：**

- 使用与任务**相关**的示例
- 示例应该**多样**且**高质量**
- 一个小错误可能会混淆模型
- 包含**边缘情况**以获得稳健输出

### 4.3 系统提示词、上下文提示词和角色提示词

这三种技术引导大语言模型如何生成文本，关注不同方面：

**系统提示词**设置整体上下文和目的——模型应该做什么的"大局观"。系统提示词定义模型的基本能力，可以指定输出要求如格式或安全指南。

**角色提示词**为模型分配特定的角色或身份，影响输出风格、声音和个性。例如，要求模型"扮演旅游向导"帮助它生成与该角色的知识和行为一致的回复。你还可以指定风格：对抗性、描述性、正式、幽默、励志、有说服力等。

**上下文提示词**提供与当前任务相关的具体细节或背景信息，帮助模型理解细微差别并相应调整回复。这确保模型快速理解你的请求并生成更准确、更相关的回复。

这三种类型之间可能有相当大的重叠。

**结构化输出（如 JSON）的好处：**

- 无需手动创建格式
- 数据可以按排序顺序返回
- **强制结构并限制幻觉**

### 4.4 退一步提示词（Step-Back Prompting）

退一步提示词通过让大语言模型首先考虑与特定任务相关的**一般问题**，然后将该答案输入后续提示词来提高性能。

这种"退一步"在尝试解决具体问题之前激活相关的**背景知识和推理**。

**好处：**

- 更准确和有洞察力的回复
- 鼓励批判性思维
- 以新颖、创造性的方式应用知识
- 可以通过关注一般原则来减轻偏见

例如，在问"为新电子游戏关卡写一个故事线"之前，先问"有哪 5 个关键设置有助于创造具有挑战性和吸引力的游戏关卡？"然后使用该回复作为你具体请求的上下文。

### 4.5 思维链（Chain of Thought，CoT）

思维链提示词通过生成**中间推理步骤**来改进推理。这帮助大语言模型生成更准确的答案，特别是对于数学和逻辑问题。

大语言模型经常在数学任务上遇到困难，因为它们是在文本上训练的，而数学需要不同的方法。在提示词中简单添加"让我们一步一步思考"可以显著提高准确性。

**CoT 的优势：**

- 低成本但非常有效
- 适用于现成的大语言模型（无需微调）
- 提供**可解释性**——你可以看到推理步骤
- 提高不同大语言模型版本之间的**稳健性**

**劣势：**

- 更多输出 token = 更高成本和更长响应时间

**少样本 CoT**更加强大——提供包含推理步骤的示例，而不仅仅是最终答案。

**CoT 的良好候选任务：**

- 代码生成（将请求分解为步骤）
- 合成数据创建
- 任何可以通过"边说边想"解决的任务

**CoT 最佳实践：**

- 将答案放在推理之后
- 将最终答案与推理分开提取
- 将 temperature 设置为 0（CoT 基于贪婪解码）

### 4.6 自洽性（Self-Consistency）

虽然大语言模型已显示出令人印象深刻的成功，但它们的推理能力往往有限。CoT 有帮助但使用简单的"贪婪解码"。**自洽性**结合采样和多数投票以获得更好的结果。

**工作原理：**

1. **生成多样的推理路径**：多次提供相同提示词，使用高 temperature 鼓励不同方法
2. 从每个生成的回复中**提取答案**
3. **选择最常见的答案**（多数投票）

自洽性给出答案正确的**伪概率可能性**，但显然由于多次运行成本更高。

### 4.7 思维树（Tree of Thoughts，ToT）

思维树通过允许大语言模型**同时探索多条不同的推理路径**来**泛化** CoT，而不是遵循单一线性链。

**工作原理：**

- 维护一棵"思维树"
- 每个思维是作为中间步骤的连贯语言序列
- 模型通过从不同节点分支来探索不同的推理路径

CoT 从输入到输出遵循单一线性路径，而 ToT 在汇聚到最终输出之前分支出多条并行路径。

ToT 特别适合**需要探索的复杂任务**。

### 4.8 ReAct（推理+行动）

ReAct 是一种使大语言模型能够使用**自然语言推理结合外部工具**（搜索、代码解释器、API）解决复杂任务的范式。这允许大语言模型执行与外部 API 交互以检索信息等操作。

**ReAct 是迈向智能体建模的第一步。**

ReAct 模仿人类的操作方式：我们口头推理并采取行动获取信息。

**工作原理：**

1. 大语言模型对问题进行推理并生成计划
2. 执行计划中的行动
3. 观察结果
4. 更新推理并生成新计划
5. 继续直到达成解决方案

例如，当被问到"Metallica 乐队成员有多少孩子？"时，ReAct 智能体会搜索乐队成员是谁，然后搜索每个成员的孩子，然后汇总结果。

### 4.9 自动提示词工程（Automatic Prompt Engineering，APE）

撰写提示词可能很复杂。**自动提示词工程**通过使用模型为你撰写提示词来自动化这一过程。

**过程：**

1. **生成**：让模型生成输出变体
2. **评估**：使用指标（BLEU、ROUGE）对候选进行评分
3. **选择**：选择得分最高的候选
4. **迭代**：调整并再次评估

这对于生成训练数据变体或找到最优提示词表述特别有用。

---

## 5. 代码提示词

Gemini 和其他大语言模型可以帮助用任何编程语言编写代码。这可以显著加速开发。

### 5.1 编写代码的提示词

你可以通过描述你需要什么来要求大语言模型编写代码。明确指定编程语言和所需功能。

**重要提示**：由于大语言模型无法真正推理并可能重复训练数据模式，在使用之前始终**阅读和测试生成的代码**。

### 5.2 解释代码的提示词

在团队中工作时，你经常需要阅读他人的代码。大语言模型可以通过分解每个部分并描述其功能来帮助解释不熟悉的代码。

### 5.3 翻译代码的提示词

需要在语言之间转换代码？大语言模型可以将代码从一种编程语言翻译到另一种，同时保持相同的功能。

### 5.4 调试和审查代码的提示词

大语言模型可以通过从错误消息中识别 bug 并建议修复来帮助调试代码。它们还可以推荐改进，如更好的错误处理、更清晰的语法或边缘情况处理。

### 5.5 多模态提示词呢？

代码提示词使用常规语言模型。**多模态提示词**是一个单独的概念——它指的是根据模型的能力和任务使用**多种输入格式**（文本、图像、音频、代码）。

---

## 6. 最佳实践

找到正确的提示词需要反复调试。使用这些最佳实践成为专家。

### 6.1 提供示例

**最重要的**最佳实践。少样本提示词非常有效，因为示例是强大的教学工具。它们展示期望的输出，允许模型学习并相应地调整其生成。这就像给模型一个参考点或目标。

### 6.2 设计要简洁

提示词应该对你和模型都**简洁、清晰、易于理解**。

**经验法则**：如果对你来说很困惑，对模型来说可能也很困惑。

**使用动作动词**：行动、分析、分类、比较、创建、描述、定义、评估、提取、查找、生成、识别、列出、组织、解析、预测、推荐、检索、重写、选择、总结、翻译、撰写

### 6.3 明确输出要求

简洁的指令可能不足以引导大语言模型，或者可能太笼统。具体细节帮助模型专注于相关内容。不要说"生成一篇关于游戏机的博客文章"，而要说"生成一篇 3 段的博客文章，介绍前 5 名游戏机。博客文章应该信息丰富且引人入胜，用对话式风格撰写。"

### 6.4 用指令而非约束

**指令**告诉模型要做什么。**约束**告诉模型不要做什么。

研究表明**正面指令比约束更有效**。这与人类更喜欢正面指令而非不要做什么的列表是一致的。

**为什么指令更有效：**

- 直接传达期望结果
- 给予灵活性并鼓励创意
- 约束可能让模型猜测
- 约束列表可能相互冲突

**何时使用约束**：防止有害或有偏见的内容，或需要严格输出格式时。

### 6.5 控制最大 Token 长度

要控制响应长度，在配置中设置最大 token 限制，或在提示词中明确请求特定长度（例如，"用一条推文长度的消息解释量子物理"）。

### 6.6 在提示词中使用变量

通过使用可针对不同输入更改的变量，使提示词动态且可重用。这节省时间，使集成更容易，并允许你使用相同的提示词模板处理不同的输入。

### 6.7 尝试不同的输入格式和写作风格

不同的模型、配置、格式和用词选择会产生不同的结果。尝试不同的风格、用词和提示词类型（零样本、少样本、系统提示词）。

同一目标可以表述为问题、陈述或指令——尝试不同的方法。

### 6.8 少样本分类时混合类别

在做分类任务时，在示例中**混合可能的响应类别**。否则，你可能会对特定顺序过拟合。

通过混合类别，你确保模型学会识别**每个类别的关键特征**，而不是记住示例顺序。

**好的经验法则**：从**6 个少样本示例**开始，然后测试准确性。

### 6.9 适应模型更新

关注模型架构变化、新增数据和新能力。尝试更新的模型版本并调整提示词以利用新功能。

### 6.10 尝试不同的输出格式

对于非创意任务（提取、选择、解析、排序、排名、分类），尝试**结构化输出格式**如 JSON 或 XML。这强制结构并限制幻觉。

### 6.11 与其他提示词工程师一起实验

如果你需要好的提示词，让**多人**尝试。当每个人都遵循最佳实践时，你会看到不同方法之间的性能差异。协作带来更好的提示词。

### 6.12 记录各种提示词尝试

**详细记录你的提示词尝试**，这样你可以学习什么有效什么无效。

提示词输出可能因模型不同、采样设置不同、同一模型的不同版本，甚至相同提示词的小格式或用词差异而不同。

**为每次尝试跟踪这些字段：**

- 提示词的名称和版本
- 目标（一句话）
- 模型名称和版本
- 配置（Temperature、Token 限制、Top-K、Top-P）
- 完整提示词文本
- 输出
- 结果（OK / NOT OK / SOMETIMES OK）
- 反馈笔记

**其他提示：**

- 跟踪迭代版本
- 将提示词保存在与代码分开的文件中以便于维护
- 依赖自动化测试和评估程序

---

## 7. 总结

这份白皮书涵盖了一套全面的**提示词技术**：

- 零样本和少样本提示词
- 系统、角色和上下文提示词
- 退一步提示词
- 思维链（CoT）
- 自洽性
- 思维树（ToT）
- ReAct（推理+行动）
- 自动提示词工程

**需要记住的关键原则：**

1. 提示词工程是**迭代的**——撰写、测试、分析、记录、改进、重复
2. **示例很强大**——少样本通常胜过巧妙的零样本
3. **思维链解锁推理**——"让我们一步一步思考"
4. **配置很重要**——temperature、top-K、top-P 显著影响输出
5. **记录一切**——重新访问提示词时你会需要它
6. **彻底测试**——特别是代码，始终验证输出
7. 当你更改模型或配置时，**回去重新实验**

---

_本文总结了 Google"提示词工程"白皮书（Lee Boonstra，2024 年 9 月）的核心见解。_
